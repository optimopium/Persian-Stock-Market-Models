{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Process Twitter Datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c41eb9a54019e50b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbaa7b8158360cb9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file already exists. No further processing is done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "output_file_path = \"./files/twitter_text.csv\"\n",
    "\n",
    "# Check if the output file already exists\n",
    "if os.path.exists(output_file_path):\n",
    "    print(\"Output file already exists. No further processing is done.\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        telegram_p1 = pd.read_csv(\"./files/telegram_data.csv\", \n",
    "                                  header=None, \n",
    "                                  quotechar='\"', \n",
    "                                  engine='python', \n",
    "                                  error_bad_lines=False)\n",
    "\n",
    "        # Extract the column with index 11\n",
    "        text_column = telegram_p1.loc[:, 11]\n",
    "\n",
    "        # Save the extracted column to a new CSV file\n",
    "        text_column.to_csv(output_file_path, index=False, header=['text'])\n",
    "        print(\"Text column saved successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error processing file:\", e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T14:41:49.698841Z",
     "start_time": "2023-12-01T14:41:49.647851Z"
    }
   },
   "id": "6cd5fb1e258d01f2"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "twitter_text = pd.read_csv(\"./files/twitter_text.csv\", header=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T14:41:52.400111Z",
     "start_time": "2023-12-01T14:41:49.653563Z"
    }
   },
   "id": "f30fc7476426eb50"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before dropping Nans: 590127\n",
      "Length after dropping Nans: 506952\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length before dropping Nans: {len(twitter_text)}\")\n",
    "twitter_text.dropna(inplace=True)\n",
    "print(f\"Length after dropping Nans: {len(twitter_text)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T14:41:52.493764Z",
     "start_time": "2023-12-01T14:41:52.471160Z"
    }
   },
   "id": "c0f988694f7e9a2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Example:**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abc3898db0ba37ab"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ÙÙˆØ±ÛŒ \n",
      "\n",
      "Ù…Ù‡Ù…  \n",
      "Ø¯Ø± Ø¬Ù„Ø³Ù‡ Ù‡ÛŒØ£Øª Ø¯ÙˆÙ„Øª ØµÙˆØ±Øª Ú¯Ø±ÙØªØ›\n",
      "\n",
      "ğŸ”¹Ø§ØµÙ„Ø§Ø­ Ù„Ø§ÛŒØ­Ù‡ Ø¨ÙˆØ¯Ø¬Ù‡ Ø³Ø§Ù„ Û±Û´Û°Û± Ø¯Ø±Ø®ØµÙˆØµ Ù†Ø­ÙˆÙ‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ø±Ø® Ú¯Ø§Ø² Ø®ÙˆØ±Ø§Ú© Ù¾ØªØ±ÙˆØ´ÛŒÙ…ÛŒâ€ŒÙ‡Ø§\n",
      "\n",
      "ğŸ”¹ØªØµÙˆÛŒØ¨ Ø¢ÛŒÛŒÙ† Ù†Ø§Ù…Ù‡ Ø§Ø¬Ø±Ø§ÛŒÛŒ Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ù…Ø§Ù„ÛŒØ§Øª Ùˆ Ø¹ÙˆØ§Ø±Ø¶ Ú©Ø§Ù„Ø§Ù‡Ø§ÛŒ Ù‡Ù…Ø±Ø§Ù‡ Ù…Ø³Ø§ÙØ±Ø§Ù† Ùˆ Ø¨Ù„ÛŒØª Ù¾Ø±ÙˆØ§Ø²Ù‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒ Ú¯Ø±Ø¯Ø´Ú¯Ø±Ø§Ù† Ø®Ø§Ø±Ø¬ÛŒ Ø®Ø±ÛŒØ¯Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ø§Ø² Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§ÛŒÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ\n",
      "\n",
      "ğŸ”¹Ù‡ÛŒØ£Øª ÙˆØ²ÛŒØ±Ø§Ù† Ø¯Ø± Ø±Ø§Ø³ØªØ§ÛŒ Ù…ØµÙˆØ¨Ù‡ Ø³ØªØ§Ø¯ Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø¯ÙˆÙ„Øª Ø¯Ø±Ø®ØµÙˆØµ Ù†Ø­ÙˆÙ‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ø±Ø® Ú¯Ø§Ø² Ø®ÙˆØ±Ø§Ú© Ù¾ØªØ±ÙˆØ´ÛŒÙ…ÛŒâ€ŒÙ‡Ø§ØŒ Ø¨Ø§ Ø§ØµÙ„Ø§Ø­ Ù„Ø§ÛŒØ­Ù‡ Ø¨ÙˆØ¯Ø¬Ù‡ Ø³Ø§Ù„ Û±Û´Û°Û± Ú©Ù„ Ú©Ø´ÙˆØ± Ø¯Ø± Ø§ÛŒÙ† Ø®ØµÙˆØµ Ù…ÙˆØ§ÙÙ‚Øª Ú©Ø±Ø¯.\n",
      "\n",
      "ğŸ”¹Ø¬Ù„Ø³Ù‡ Ù‡ÛŒØ£Øª Ø¯ÙˆÙ„Øª Ø¨Ø¹Ø¯ Ø§Ø² Ø¸Ù‡Ø± Ø§Ù…Ø±ÙˆØ² ÛŒÚ©Ø´Ù†Ø¨Ù‡ Ø¨Ù‡ Ø±ÛŒØ§Ø³Øª Ø¢ÛŒØª Ø§Ù„Ù„Ù‡ Ø³ÛŒØ¯ Ø§Ø¨Ø±Ø§Ù‡ÛŒÙ… Ø±Ø¦ÛŒØ³ÛŒ Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯ Ùˆ Ø¯Ø± Ø¢Ù† Ù…ØµÙˆØ¨ Ø´Ø¯ ÙØ±Ù…ÙˆÙ„ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ø±Ø® Ú¯Ø§Ø² Ø®ÙˆØ±Ø§Ú© Ù¾ØªØ±ÙˆØ´ÛŒÙ…ÛŒâ€ŒÙ‡Ø§ Ù…Ø§Ù†Ù†Ø¯ Ø³Ø§Ù„ Û±Û´Û°Û° Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ø§Ø¹Ù…Ø§Ù„ Ú¯Ø±Ø¯Ø¯.\n",
      "\n",
      "ğŸ”¹Ù‡Ù…Ú†Ù†ÛŒÙ† Ù†Ø±Ø® Ú¯Ø§Ø² Ø³ÙˆØ®Øª Ù¾ØªØ±ÙˆØ´ÛŒÙ…ÛŒâ€ŒÙ‡Ø§ØŒ Ù¾Ø§Ù„Ø§ÛŒØ´Ú¯Ø§Ù‡â€ŒÙ‡Ø§ Ùˆ ØµÙ†Ø§ÛŒØ¹ Ù¾Ø§ÛŒÛŒÙ† Ø¯Ø³ØªÛŒØŒ Ù…Ø¬ØªÙ…Ø¹â€ŒÙ‡Ø§ÛŒ Ø§Ø­ÛŒØ§ÛŒ ÙÙˆÙ„Ø§Ø¯ Ùˆ Ù…ØµØ§Ø±Ù Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÛŒÙˆØªÛŒÙ„ÛŒØªÛŒ Ø´Ø§Ù…Ù„ Ø¨Ø±Ù‚ØŒ Ø¢Ø¨ØŒ Ø§Ú©Ø³ÛŒÚ˜Ù† Ùˆ ØºÛŒØ±Ù‡ Ø¢Ù†Ù‡Ø§ Ù…Ø¹Ø§Ø¯Ù„ Û´Û° Ø¯Ø±ØµØ¯ Ù†Ø±Ø® Ø®ÙˆØ±Ø§Ú© Ú¯Ø§Ø² Ù¾ØªØ±ÙˆØ´ÛŒÙ…ÛŒ Ù…ÛŒØ¨Ø§Ø´Ø¯ Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† ØµÙ†Ø§ÛŒØ¹ Ø³ÛŒÙ…Ø§Ù† Ùˆ Ø³Ø§ÛŒØ± ØµÙ†Ø§ÛŒØ¹ Ù…Ø¹Ø§Ø¯Ù„ Û±Û° Ø¯Ø±ØµØ¯ Ù†Ø±Ø® Ø®ÙˆØ±Ø§Ú© Ù¾ØªØ±ÙˆØ´ÛŒÙ…ÛŒâ€ŒÙ‡Ø§ ØªØ¹ÛŒÛŒÙ† Ù…ÛŒÚ¯Ø±Ø¯Ø¯. Ù…Ø¹Ø§Ø¯Ù„ Ú©Ø§Ù‡Ø´ Ø¯Ø± Ù…Ù†Ø§Ø¨Ø¹ Ù†Ø§Ø´ÛŒ Ø§Ø² Ø§ÛŒÙ† ØªØµÙ…ÛŒÙ… Ù†ÛŒØ² Ù…ÛŒØ¨Ø§ÛŒØ³ØªÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§Ù‡Ø´ Ø¯Ø± Ù…ØµØ§Ø±Ù ØªØ¨ØµØ±Ù‡ (Û±Û´) Ø§Ø¹Ù…Ø§Ù„ Ú¯Ø±Ø¯Ø¯.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{twitter_text.iloc[0].text}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T14:41:52.493984Z",
     "start_time": "2023-12-01T14:41:52.492406Z"
    }
   },
   "id": "3dfcb9effd741b09"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apply Rule-based Method to Create Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dadb23eabbac4fea"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data.loaders import load_symbols_from_csv\n",
    "from symbol_detector.rule_based_detector.detector import extract_symbol_spans\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "\n",
    "# Load keywords\n",
    "symbols_data = load_symbols_from_csv(columns=['symbol'], path='./files/symbols.csv')\n",
    "keywords = [d['symbol'] for d in symbols_data]\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "def tag_text(row, input_keywords, input_tokenizer):\n",
    "    # Ensure the input is a string\n",
    "    if not isinstance(row['text'], str):\n",
    "        return [], row['text']\n",
    "\n",
    "    text = row['text']\n",
    "    spans = extract_symbol_spans(text, input_keywords)\n",
    "\n",
    "    # Tokenize text and initialize labels\n",
    "    encoded = input_tokenizer.encode_plus(text, return_offsets_mapping=True, add_special_tokens=False)\n",
    "    tokens = encoded['input_ids']\n",
    "    offsets = encoded['offset_mapping']\n",
    "    labels = ['O'] * len(tokens)\n",
    "\n",
    "    # Mark tokens that are part of a keyword\n",
    "    for span_info in spans:\n",
    "        span_start, span_end = span_info['span']\n",
    "        for idx, (start, end) in enumerate(offsets):\n",
    "            if start < span_end and end > span_start:\n",
    "                labels[idx] = 'KEYWORD'\n",
    "\n",
    "    return labels, input_tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "# Function to process a batch and save intermediate results\n",
    "def process_batch(batch, batch_index, input_keywords, input_tokenizer):\n",
    "    tqdm.pandas(desc=f\"Processing Batch {batch_index}\")\n",
    "    batch[['labels', 'tokens']] = batch.progress_apply(lambda row: tag_text(row, input_keywords, input_tokenizer), axis=1, result_type='expand')\n",
    "    batch.to_csv(f'./files/intermediate_artifacts/intermediate_batch_{batch_index}.csv', index=False)\n",
    "    print(f\"Batch {batch_index} processed and saved.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T14:41:54.003506Z",
     "start_time": "2023-12-01T14:41:52.499115Z"
    }
   },
   "id": "4aee8fd3d051a03c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "506952"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twitter_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T14:41:54.007163Z",
     "start_time": "2023-12-01T14:41:54.004157Z"
    }
   },
   "id": "bc756e7f7bf509bf"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 0:   0%|          | 10/10000 [00:00<10:15, 16.23it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (701 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 5:   0%|          | 3/10000 [00:00<06:05, 27.38it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 1:   0%|          | 14/10000 [00:00<09:59, 16.65it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (874 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 0:   0%|          | 19/10000 [00:01<09:36, 17.32it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (770 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 7:   0%|          | 37/10000 [00:02<09:47, 16.97it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (736 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 1:   1%|          | 56/10000 [00:04<11:37, 14.25it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 2:   1%|          | 122/10000 [00:07<08:43, 18.86it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 7:   2%|â–         | 193/10000 [00:11<12:35, 12.99it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [13:28<00:00, 12.36it/s]\n",
      "Processing Batch 5:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9784/10000 [13:31<00:21, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [13:33<00:00, 12.29it/s]\n",
      "Processing Batch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9967/10000 [13:36<00:03,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [13:40<00:00, 12.19it/s]\n",
      "Processing Batch 7:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9905/10000 [13:42<00:10,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [13:44<00:00, 12.13it/s]\n",
      "Processing Batch 7:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9942/10000 [13:46<00:06,  9.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9980/10000 [13:47<00:01, 10.15it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [13:50<00:00, 12.05it/s]\n",
      "Processing Batch 9:   2%|â–         | 165/10000 [00:16<15:52, 10.33it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [13:52<00:00, 12.01it/s]\n",
      "Processing Batch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [13:53<00:00, 12.00it/s]\n",
      "Processing Batch 8:   2%|â–         | 237/10000 [00:23<15:47, 10.30it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (641 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 11:   1%|          | 83/10000 [00:08<16:46,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 14:   0%|          | 3/10000 [00:00<09:57, 16.73it/s]]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 12:   1%|          | 52/10000 [00:05<18:02,  9.19it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 2:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9852/10000 [14:31<00:16,  8.79it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [14:50<00:00, 11.23it/s]\n",
      "Processing Batch 8:   8%|â–Š         | 780/10000 [01:21<16:54,  9.09it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 13:   6%|â–Œ         | 559/10000 [01:01<17:22,  9.06it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (971 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 13:   8%|â–Š         | 816/10000 [01:35<17:31,  8.73it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 12:   9%|â–‰         | 943/10000 [01:51<17:01,  8.86it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (592 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batch 8:  13%|â–ˆâ–        | 1253/10000 [02:17<15:58,  9.13it/s]]\n",
      "Processing Batch 12:  10%|â–‰         | 972/10000 [01:55<17:56,  8.39it/s]\n",
      "Processing Batch 10:  11%|â–ˆâ–        | 1147/10000 [02:06<16:12,  9.10it/s]\n",
      "Processing Batch 13:  10%|â–‰         | 966/10000 [01:53<17:39,  8.52it/s]\n",
      "Processing Batch 9:  12%|â–ˆâ–        | 1197/10000 [02:12<16:14,  9.03it/s]\n",
      "Processing Batch 11:  11%|â–ˆ         | 1054/10000 [02:01<17:14,  8.65it/s]\n",
      "Processing Batch 15:   5%|â–         | 462/10000 [00:55<19:05,  8.32it/s]\n",
      "Processing Batch 14:  10%|â–‰         | 960/10000 [01:52<17:41,  8.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Process in batches\u001B[39;00m\n\u001B[1;32m      5\u001B[0m num_batches \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mlen\u001B[39m(twitter_text) \u001B[38;5;241m+\u001B[39m batch_size \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m batch_size\n\u001B[0;32m----> 6\u001B[0m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtwitter_text\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m:\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeywords\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnum_batches\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Consolidate intermediate results\u001B[39;00m\n\u001B[1;32m      9\u001B[0m consolidated \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./files/intermediate_artifacts/intermediate_batch_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_batches)])\n",
      "File \u001B[0;32m~/PycharmProjects/stock_market_sentiment/venv/lib/python3.9/site-packages/joblib/parallel.py:1952\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1946\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[1;32m   1947\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[1;32m   1948\u001B[0m \u001B[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001B[39;00m\n\u001B[1;32m   1949\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[1;32m   1950\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 1952\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/stock_market_sentiment/venv/lib/python3.9/site-packages/joblib/parallel.py:1595\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[0;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[1;32m   1592\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[1;32m   1594\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1595\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[1;32m   1597\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[1;32m   1598\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[1;32m   1599\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[1;32m   1600\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[1;32m   1601\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/stock_market_sentiment/venv/lib/python3.9/site-packages/joblib/parallel.py:1707\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1702\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[1;32m   1703\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[1;32m   1704\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[1;32m   1705\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[1;32m   1706\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[0;32m-> 1707\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1708\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m   1710\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[1;32m   1711\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[1;32m   1712\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Assume twitter_text is your input DataFrame\n",
    "batch_size = 10_000  # Adjust this based on your dataset size and memory capacity\n",
    "\n",
    "# Process in batches\n",
    "num_batches = (len(twitter_text) + batch_size - 1) // batch_size\n",
    "Parallel(n_jobs=-1)(delayed(process_batch)(twitter_text.iloc[i * batch_size:(i + 1) * batch_size], i, keywords, tokenizer) for i in range(num_batches))\n",
    "\n",
    "# Consolidate intermediate results\n",
    "consolidated = pd.concat([pd.read_csv(f'./files/intermediate_artifacts/intermediate_batch_{i}.csv') for i in range(num_batches)])\n",
    "consolidated.to_csv('./files/twitter_tagged.csv', index=False)\n",
    "print(\"All batches processed. Final output saved.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T14:57:46.934519Z",
     "start_time": "2023-12-01T14:41:54.008328Z"
    }
   },
   "id": "592e34345dcda43b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TEST:**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e5fd2e438a59eef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_tokenizer(index):\n",
    "    text = twitter_text.iloc[110].text\n",
    "    print(f\"Text:\\n {text}\")\n",
    "    print(f\"Extract Keywords:\\n {extract_symbol_spans(text, keywords)}\")\n",
    "    print(f\"Tokenized:\")\n",
    "    labels, tokens = tag_text(twitter_text.iloc[110], keywords, tokenizer)\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label == \"KEYWORD\":\n",
    "            print(token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T14:57:46.937763Z"
    }
   },
   "id": "22ce930c206e197c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_tokenizer(110)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T14:57:46.942688Z",
     "start_time": "2023-12-01T14:57:46.940731Z"
    }
   },
   "id": "2db738d7c0263093"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PyTorch Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23f6453dd79b9358"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, filename, input_tokenizer, max_len):\n",
    "        self.data = pd.read_csv(filename)\n",
    "        self.tokenizer = input_tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_map = {'O': 0, 'KEYWORD': 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = row['text']\n",
    "        word_labels = eval(row['labels'])  # Assuming this returns a list of labels for each word\n",
    "\n",
    "        # Tokenize the text and align labels with tokens\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_offsets_mapping=True,  # We need this to align labels with tokens\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Create a new list for labels aligned with tokenized input\n",
    "        labels = []\n",
    "        last_word_idx = -1\n",
    "        for offset in encoding['offset_mapping'].squeeze().tolist():\n",
    "            # Check if we are at the start of a new word\n",
    "            if offset[0] == 0 or last_word_idx != offset[0]:\n",
    "                labels.append(self.label_map[word_labels.pop(0)] if word_labels else self.label_map['O'])\n",
    "            else:\n",
    "                labels.append(labels[-1])  # Copy label from previous subword token\n",
    "            last_word_idx = offset[1]\n",
    "\n",
    "        # Padding labels if necessary\n",
    "        labels = labels[:self.max_len]  # Truncate to max_len\n",
    "        labels += [self.label_map['O']] * (self.max_len - len(labels))  # Pad if needed\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
    "dataset = NERDataset(filename='./files/twitter_tagged.csv', input_tokenizer=tokenizer, max_len=128)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T14:57:46.943440Z"
    }
   },
   "id": "d2e760a5ee683267"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T14:57:46.945927Z"
    }
   },
   "id": "adf0d43a72862065"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next(iter(dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T14:57:46.948368Z"
    }
   },
   "id": "7a94735b44e8bfba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6784c0aa497f988"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Assuming dataset is an instance of NERDataset\n",
    "# Define the size of your training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T14:57:46.950513Z"
    }
   },
   "id": "941df370e269e049"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69b2476cdef5bf4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaForTokenClassification, AdamW\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=2)  # Adjust num_labels based on your task\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3  # How many epochs to wait after last time validation loss improved.\n",
    "best_loss = np.inf\n",
    "counter = 0\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T14:57:46.952726Z"
    }
   },
   "id": "3c57026d170fead3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Add a progress bar for the training loop\n",
    "    train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1} Training\", leave=False)\n",
    "    for batch in train_progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        train_progress_bar.set_postfix({'Train Loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # Add a progress bar for the validation loop\n",
    "    val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch + 1} Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            val_progress_bar.set_postfix({'Val Loss': loss.item()})\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f'Epoch {epoch + 1} - Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Early Stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T14:57:46.954690Z"
    }
   },
   "id": "5e8528b77d089764"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5ca2b28a68ca446"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model_save_path = './models/xlm_roberta_ner_model.pt'\n",
    "torch.save(model.state_dict(), model_save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T14:57:46.956445Z"
    }
   },
   "id": "8931a1893a051788"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and Inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3490ff1fcd1f7cf4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_model(model_path, num_labels):\n",
    "    # Initialize the model\n",
    "    stock_ner_model = XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=num_labels)\n",
    "    stock_ner_model.load_state_dict(torch.load(model_path))\n",
    "    stock_ner_model.eval()\n",
    "    return stock_ner_model\n",
    "\n",
    "def predict(text, model, tokenizer, max_len=128):\n",
    "    # Tokenize the input text\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Move tensors to the same device as the model\n",
    "    input_ids = encoding['input_ids'].to(model.device)\n",
    "    attention_mask = encoding['attention_mask'].to(model.device)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Convert probabilities to label IDs\n",
    "    predictions = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "    # Convert label IDs to label names\n",
    "    label_map = {0: 'O', 1: 'KEYWORD'}  # Update this based on your label mapping\n",
    "    predicted_labels = [label_map[label_id] for label_id in predictions.squeeze().tolist()]\n",
    "\n",
    "    # Convert input IDs to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "\n",
    "    return list(zip(tokens, predicted_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T14:57:46.958229Z"
    }
   },
   "id": "45385cac288f7c53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Usage\n",
    "model = load_model('./models/xlm_roberta_ner_model.pt', num_labels=2)\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T14:57:46.959981Z"
    }
   },
   "id": "44b5ab09b803a4f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_text = \"Ø®Ø³Ø§Ù¾Ø§ Ø³Ù‡Ù… Ø¨Ø³ÛŒØ§Ø± Ø®ÙˆØ¨ÛŒ Ø§Ø³Øª\"\n",
    "predictions = predict(sample_text, model, tokenizer)\n",
    "print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T14:57:46.962046Z"
    }
   },
   "id": "7d7a1e606cde420"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_text = \"Ø¨Ø§ Ø®Ø±ÛŒØ¯ Ø¨ÙˆØ±Ø³ Ø´Ø³ØªØ§ Ø³ÙˆØ¯ Ø®ÙˆØ¨ÛŒ Ú©Ø±Ø¯ÛŒÙ…\"\n",
    "predictions = predict(sample_text, model, tokenizer)\n",
    "print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T14:57:46.964167Z"
    }
   },
   "id": "e09bb8770ce28d9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T14:57:46.965806Z"
    }
   },
   "id": "89566dbd738b65b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
